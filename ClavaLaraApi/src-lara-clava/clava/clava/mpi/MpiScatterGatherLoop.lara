import clava.mpi.MpiUtils;

import clava.Clava;
import clava.ClavaJoinPoints;

/**
 * Applies an MPI scatter-gather strategy to loops.
 * @class
 */
function MpiScatterGatherLoop($loop) {
	this._$loop = $loop;
	// Inputs + access pattern
	// Outputs + access pattern
}

MpiScatterGatherLoop.prototype.addInput = function(varName, accessPattern) {

	// Check if loop contains a reference to the variable
	var firstVarref = undefined;
	for(var $varref of this._$loop.descendants('varref')) {
		if($varref.name === varName) {
			firstVarref = $varref;
			break;
		}
	}
	
	if(firstVarref === undefined) {
		throw "Could not find a reference to the variable '"+varName+"' in the loop located at " + this._$loop.location;
	}

	println("Varref type: " + $varref.type.code);
	
	if(accessPattern === undefined) {
		
	}
	

}

/**
 * Adapts code to use the MPI strategy.
 */ 
MpiScatterGatherLoop.prototype.execute = function() {

	var $mainFunction = ClavaCode.getFunctionDefinition("main", true);
	var $mainFile = $mainFunction.ancestor("file");
	
	// Add include
	$mainFile.addInclude("mpi.h");
	
	// Add global variables
	$intType = ClavaJoinPoints.builtinType("int");
	$mainFile.addGlobal(MpiUtils.VAR_NUM_TASKS, $intType, 0);
	$mainFile.addGlobal(MpiUtils.VAR_NUM_WORKERS, $intType, 0);
	var $rankDecl = $mainFile.addGlobal(MpiUtils.VAR_RANK, $intType, 0);

	// Add MPI Worker
	$rankDecl.insertAfter(this._buildMpiWorker());	

	// Replace loop with MPI Master routine
	this._replaceLoop();
	
	// Add MPI initialization
	this._addMpiInit($mainFunction);
}


/** PRIVATE SECTION **/

MpiScatterGatherLoop._FUNCTION_MPI_WORKER = "mpi_worker";
MpiScatterGatherLoop._VAR_MPI_STATUS = "status";
MpiScatterGatherLoop._VAR_WORKER_NUM_ELEMS = "mpi_loop_num_elems";

MpiScatterGatherLoop.prototype._replaceLoop = function() {
	this._$loop.replaceWith(MpiMaster(MpiUtils.VAR_NUM_WORKERS, "", ""));
}


MpiScatterGatherLoop.prototype._buildMpiMaster = function() {

	return MpiMaster(MpiUtils.VAR_NUM_WORKERS);
}

MpiScatterGatherLoop.prototype._buildMpiWorker = function() {

	return MpiWorker(MpiScatterGatherLoop._FUNCTION_MPI_WORKER, MpiScatterGatherLoop._VAR_MPI_STATUS, MpiScatterGatherLoop._VAR_WORKER_NUM_ELEMS, "", "", "", "");
}

MpiScatterGatherLoop.prototype._addMpiInit = function($mainFunction) {

	// Add params to main, if no params
	if($mainFunction.params.length === 0) {
		$mainFunction.paramsFromStrings = ["int argc", "char** argv"];
 	}

	var numMainParams = $mainFunction.params.length;
	checkTrue(numMainParams === 2, "Expected main() function to have 2 paramters, has '" + numMainParams +"'");

	var argc = $mainFunction.params[0].name;
	var argv = $mainFunction.params[1].name;

	$mainFunction.body.insertBegin(MpiInit(argc, argv, MpiUtils.VAR_RANK, MpiUtils.VAR_NUM_TASKS, MpiUtils.VAR_NUM_WORKERS, MpiScatterGatherLoop._FUNCTION_MPI_WORKER));

}


/** CODEDEFS **/

codedef MpiInit(argc, argv, rank, numTasks, numWorkers, mpiWorker) %{
    MPI_Init(&[[argc]], &[[argv]]);
    MPI_Comm_rank(MPI_COMM_WORLD, &[[rank]]);
    MPI_Comm_size(MPI_COMM_WORLD, &[[numTasks]]);
    [[numWorkers]] = [[numTasks]] - 1;

    if([[numWorkers]] == 0) {
        std::cerr << "This program does not support working with a single process." << std::endl;
        return 1;
    }

	if([[rank]] > 0) {
		[[mpiWorker]]();
		MPI_Finalize();
		return 0;
	}
}% end

codedef MpiWorker(functionName, status, numElems, receiveData, outputDecl, loop, sendData) %{
void [[functionName]]() {
    MPI_Status [[status]];

	// Number of loop iterations
    int [[numElems]];

    MPI_Recv(&[[numElems]], 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &[[status]]);
	
	[[receiveData]]
	
	[[outputDecl]]
	
	[[loop]]
	
	[[sendData]]	
}
}% end


codedef MpiMaster(numWorkers, numIterations, masterSend, masterReceive) %{
	// Master routine
	
	// split iterations of the loop
	int clava_mpi_total_iter = [[numIterations]];
	int clava_mpi_loop_limit = clava_mpi_total_iter;
	// A better distribution calculation could be used
	int clava_mpi_num_iter = clava_mpi_total_iter / [[numWorkers]];
	int clava_mpi_num_iter_last = clava_mpi_num_iter + clava_mpi_total_iter % [[numWorkers]];
	// int clava_mpi_num_iter_last = clava_mpi_num_iter + (clava_mpi_loop_limit - (clava_mpi_num_iter * [[numWorkers]]));
	
	// send number of iterations
	for(int i=0; i<[[numWorkers]]-1; i++) {
		MPI_Send(&clava_mpi_num_iter, 1, MPI_INT, i+1, 1, MPI_COMM_WORLD);
	}
	MPI_Send(&clava_mpi_num_iter_last, 1, MPI_INT, [[numWorkers]], 1, MPI_COMM_WORLD);
	
	
	[[masterSend]]
	
	[[masterReceive]]
	
	MPI_Finalize();
}% end